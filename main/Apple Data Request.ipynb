{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is only compatible with a python 3.4 kernel and the notebook must be opened with ipython 3.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdb import set_trace as debug\n",
    "from pandas.io.parsers import read_csv\n",
    "from IPython.parallel import Client,require\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "import gzip\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--> RAW_FILE_DIR with the directory that contains google_play__main.json.gz\n",
    "#--> OUT_DIR with the directory of where you want the processed files to go\n",
    "#--> DEBUG whether to print out created CSV files or not\n",
    "#--> COMPRESS_LEVEL gzip compression level. warning: severely impacts runtime.\n",
    "#--> LINE_LIMIT how many lines to iterate through in the raw file. For debugging.\n",
    "DUMP_DATE = '2015_02_26_23_12'\n",
    "RAW_FILE_DIR = '/data'\n",
    "OUT_DIR = '/raid/out'\n",
    "DEBUG = 0\n",
    "COMPRESS_LEVEL = 1\n",
    "LINE_LIMIT = -1\n",
    "PARALLEL = 1\n",
    "PIGZ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't change these!\n",
    "RAW_FILE = '{}/apple_ios__main.json.gz'.format(RAW_FILE_DIR)\n",
    "OUT_BASE = OUT_DIR+'/{}__apple_ios__main__'+DUMP_DATE+'.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PIGZ:\n",
    "    @require(gzip)\n",
    "    def iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT):\n",
    "        return gzip.iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT)\n",
    "        \n",
    "else:\n",
    "    @require(gzip,ujson)\n",
    "    def iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT):\n",
    "        with gzip.open(filename,'rt') as file_iter:\n",
    "            for c,line in enumerate(file_iter):\n",
    "                if c > LINE_LIMIT and LINE_LIMIT>0:\n",
    "                    break\n",
    "                if isinstance(line,str):\n",
    "                    if len(line)>0:        \n",
    "                        out = ujson.loads(line)\n",
    "                        if isinstance(out,dict):\n",
    "                            if 'app_id' in out and 'timestamp' in out:\n",
    "                                yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a couple observations to inspect daw data.\n",
    "# !zcat \"$RAW_FILE\"|head -n 100 | tail -n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parallel Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 active computing engines\n",
      "{'migdata': 4}\n"
     ]
    }
   ],
   "source": [
    "if PARALLEL:\n",
    "    ipython_parallel = Client()\n",
    "    print(\"{} active computing engines\".format(len(ipython_parallel.ids)))\n",
    "\n",
    "    lbv = ipython_parallel.load_balanced_view()\n",
    "\n",
    "    map = lambda f,itertable:lbv.map(f,itertable,\\\n",
    "    block =False,\\\n",
    "    ordered =False)\n",
    "\n",
    "    @require('socket')\n",
    "    def host(dummy):\n",
    "        return socket.gethostname()\n",
    "    \n",
    "    nodes = list(lbv.map(host,ipython_parallel.ids))\n",
    "\n",
    "    print(dict(Counter(nodes).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWWEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWWEBSITE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','url']\n",
    "    out_filename = out_base.format('NEWWEBSITE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        http = re.compile(r\"(http://)|(https://)\")\n",
    "        www = re.compile(r\"^www\\.\")\n",
    "        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n",
    "        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            website = json.get('websiteUrl','')\n",
    "            if isinstance(website,str):\n",
    "                website = http.sub('',website).lower()\n",
    "                website = www.sub('',website)\n",
    "                website = website.split(r\"/\")[0]\n",
    "                bad_str = bad.findall(website)\n",
    "                if bad_str:\n",
    "                    website = website.replace(bad_str[0],'')\n",
    "                if running.get(json['app_id'],'')!=website and len(website)>0:\n",
    "                    running[json['app_id']] = website\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    obs = (json['app_id'],day,website)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWWEBSITE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSUPPORTURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWSUPPORTURL(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','support_url']\n",
    "    out_filename = out_base.format('NEWSUPPORTURL')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        http = re.compile(r\"(http://)|(https://)\")\n",
    "        www = re.compile(r\"^www\\.\")\n",
    "        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n",
    "        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            website = json.get('supportUrl','')\n",
    "            if isinstance(website,str):\n",
    "                website = http.sub('',website).lower()\n",
    "                website = www.sub('',website)\n",
    "                website = website.split(r\"/\")[0]\n",
    "                bad_str = bad.findall(website)\n",
    "                if bad_str:\n",
    "                    website = website.replace(bad_str[0],'')\n",
    "                if running.get(json['app_id'],'')!=website and len(website)>0:\n",
    "                    running[json['app_id']] = website\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    obs = (json['app_id'],day,website)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSUPPORTURL(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIMILAR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWSIMILAR5(file_in,out_base,compress=1,number_similar = 5):\n",
    "    headers = ['app_id','date']+ ['related{}'.format(x) \\\n",
    "                                  for x in range(1,number_similar+1)]\n",
    "    out_filename = out_base.format('NEWSIMILAR{}'.format(number_similar))\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            related = json.get('customersAlsoBoughtApps',[])\n",
    "            if related:\n",
    "                related = sorted(map(str, related[:number_similar]))\n",
    "                joined = \",\".join(related)\n",
    "                if running.get(json['app_id'],'')!=joined:\n",
    "                    running[json['app_id']] = joined\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    csv_writer.writerow([json['app_id'],day]+ related)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NEWSIMILAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(NEWSIMILAR5)\n",
    "def NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n",
    "    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIMILAR15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@require(NEWSIMILAR5)\n",
    "def NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n",
    "    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILYRATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def DAILYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('DAILYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .strftime('%Y-%m-%d')\n",
    "            rating = json.get('ratingCountList',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],day,tuple(rating),)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],day]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(DAILYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILYRATINGSCURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def DAILYRATINGSCURRENT(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('DAILYRATINGSCURRENT')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .strftime('%Y-%m-%d')\n",
    "            rating = json.get('ratingCountList_current',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],day,tuple(rating),)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],day]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Will be empty because this variable wasn't scraped for the first couple million observations\n",
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(DAILYRATINGSCURRENT(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEKLYRATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def WEEKLYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','year','week','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('WEEKLYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            year,week = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .isocalendar()[:2]\n",
    "            rating = json.get('ratingCountList',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],year,week)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],year,week]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(WEEKLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEKLYRATINGSCURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def WEEKLYRATINGSCURRENT(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','year','week','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('WEEKLYRATINGSCURRENT')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            year,week = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .isocalendar()[:2]\n",
    "            rating = json.get('ratingCountList_current',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],year,week)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],year,week]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(WEEKLYRATINGSCURRENT(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTHLYRATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def MONTHLYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','year','month','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('MONTHLYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            dateobj = datetime.datetime.fromtimestamp(int(json['timestamp']))\n",
    "            year,month = dateobj.year,dateobj.month\n",
    "            rating = json.get('ratingCountList',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],year,month)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],year,month]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(MONTHLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWDEVELOPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWDEVELOPER(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','dev_id']\n",
    "    out_filename = out_base.format('NEWDEVELOPER')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            developer = str(json.get('artist_id',''))\n",
    "            developer = developer.strip()\n",
    "            if running.get(json['app_id'],'')!=developer \\\n",
    "                        and len(developer)>0 \\\n",
    "                        and developer.isdigit():\n",
    "                running[json['app_id']] = developer\n",
    "                day = datetime.datetime.\\\n",
    "                fromtimestamp(int(json['timestamp']))\\\n",
    "                        .strftime('%Y-%m-%d').strip()\n",
    "                obs = (json['app_id'],day,developer)\n",
    "                csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWDEVELOPER(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWCATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWCATEGORY(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date']+ ['category{}'.format(x) \\\n",
    "                                  for x in range(1,5)]\n",
    "    out_filename = out_base.format('NEWCATEGORY')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            category = json.get('categories',[])[:4]\n",
    "            joined = \",\".join(category)\n",
    "            if running.get(json['app_id'],'')!=joined:\n",
    "                running[json['app_id']] = joined\n",
    "                day = datetime.datetime.\\\n",
    "                fromtimestamp(int(json['timestamp']))\\\n",
    "                        .strftime('%Y-%m-%d').strip()\n",
    "                csv_writer.writerow([json['app_id'],day]+ category)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWCATEGORY(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWPRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWPRICE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','price']\n",
    "    out_filename = out_base.format('NEWPRICE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            price = str(json.get('price','')).replace(\"$\",\"\")\n",
    "            if running.get(json['app_id'],'')!=price:\n",
    "                running[json['app_id']] = price\n",
    "                day = datetime.datetime.\\\n",
    "                fromtimestamp(int(json['timestamp']))\\\n",
    "                        .strftime('%Y-%m-%d').strip()\n",
    "                obs = (json['app_id'],day,price)\n",
    "                csv_writer.writerow(obs) \n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWPRICE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALLVERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def ALLVERSIONS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','version_string',\\\n",
    "#                'release_notes'\\\n",
    "              ]\n",
    "    out_filename = out_base.format('ALLVERSIONS')\n",
    "    unique_ver = set()\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            for each_version in json.get('version',[]):\n",
    "                # need extra code because Apple changed format\n",
    "                if 'release-date' in each_version:\n",
    "                    date = each_version.get('release-date','')\n",
    "                    date = datetime.datetime.strptime(date,'%b %d, %Y')\n",
    "                    date = date.strftime('%Y-%m-%d')\n",
    "                    version_string = each_version.get('version-string','')\n",
    "                    release_notes = each_version.get('release-notes','')\n",
    "                elif 'releaseDate' in each_version:\n",
    "                    date = each_version.get('releaseDate','')\n",
    "                    date = datetime.datetime.strptime(date,'%Y-%m-%dT%H:%M:%SZ')\n",
    "                    date = date.strftime('%Y-%m-%d')\n",
    "                    version_string = each_version.get('versionString','')\n",
    "                    release_notes = each_version.get('releaseNotes','')\n",
    "                obs_u = (json['app_id'],date,version_string)\n",
    "                if hash(obs_u) not in unique_ver:\n",
    "                    unique_ver.add(hash(obs_u))\n",
    "                    obs = (json['app_id'],date,\n",
    "                           version_string,\n",
    "#                            release_notes,\n",
    "                          )\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(ALLVERSIONS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,gzip,datetime,re,csv)\n",
    "def NEWREQ(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','requires','optimized']\n",
    "    out_filename = out_base.format('NEWREQ')\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file, dialect='excel')\n",
    "        csv_writer.writerow(headers)\n",
    "        running = {}\n",
    "        and_re = re.compile(r\"and \")\n",
    "        com_with_re = re.compile(r\"Compatible with \")\n",
    "        optimize_re = re.compile(r\"This app is optimized for \")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            requires = json.get('requirements',None)\n",
    "            if requires == None:\n",
    "                continue\n",
    "            re_split = requires.split(\".\")\n",
    "            requires = com_with_re.sub('',re_split[0]).strip()\n",
    "            requires = and_re.sub('',requires)\n",
    "            optimized = None\n",
    "            if len(re_split)>1:\n",
    "                optimized = re_split[1].strip()\n",
    "                optimized = optimize_re.sub(\"\",optimized)\n",
    "            if running.get(json['app_id'],'')!=json.get('requirements',''):\n",
    "                running[json['app_id']] = json.get('requirements','')\n",
    "                day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                        .strftime('%Y-%m-%d')\n",
    "                obs = (json['app_id'],day,requires,optimized)\n",
    "                csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWREQ(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWNAME(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'name'] # list of column names\n",
    "    out_filename = out_base.format('NEWNAME')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            name = json.get('app_name','')\n",
    "            if isinstance(name,str):\n",
    "                name = name.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=name and len(name)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = name\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', '')))), name]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWNAME(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWINAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWINAPP(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'has_inapp'] # list of column names\n",
    "    out_filename = out_base.format('NEWINAPP')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            name = json.get('hasInAppPurchases','')\n",
    "            if isinstance(name,str):\n",
    "                name = name.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=name and len(name)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = name\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', '')))), name]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWINAPP(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def DAILYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'day'] # list of column names\n",
    "    out_filename = out_base.format('DAILYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            day = str(datetime.date.fromtimestamp(json.get('timestamp','')).day)\n",
    "            if isinstance(day,str):\n",
    "                day = day.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=day and len(day)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = day\n",
    "                    obs = [json['app_id'], day]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(DAILYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTHLYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def MONTHLYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'month'] # list of column names\n",
    "    out_filename = out_base.format('MONTHLYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            month = str(datetime.date.fromtimestamp(json.get('timestamp','')).month)\n",
    "            if isinstance(month,str):\n",
    "                month = month.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=month and len(month)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = month\n",
    "                    obs = [json['app_id'], month]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(MONTHLYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEKLYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def WEEKLYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'year', 'week'] # list of column names\n",
    "    out_filename = out_base.format('WEEKLYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            date = (str(datetime.date.fromtimestamp(json.get('timestamp','')).year), str(datetime.date.fromtimestamp(json.get('timestamp','')).isocalendar()[1]))\n",
    "            if isinstance(date,str):\n",
    "                date = date.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=date and len(date)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = date\n",
    "                    obs = [json['app_id'], date[0], date[1]]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(WEEKLYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWLANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWLANGUAGES(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'language1', 'language2', 'language3', 'language4', 'language5', 'language6'] # list of column names\n",
    "    out_filename = out_base.format('NEWNAME')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            langs = tuple(sorted(tuple(json.get('languages','').split(','))))\n",
    "            if isinstance(langs,tuple):\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=langs and len(langs)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = langs\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', ''))))]\n",
    "                    [obs.append(str(x).strip()) for x in langs]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWLANGUAGES(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWSIZE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'size'] # list of column names\n",
    "    out_filename = out_base.format('NEWSIZE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            size = json.get('size','')\n",
    "            if isinstance(size,str):\n",
    "                size = size.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=size and len(size)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = size\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', '')))), size]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIZE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/out/NEWINAPP__apple_ios__main__2015_02_26_23_12.csv.gz\n",
      "/raid/out/NEWINAPP__apple_ios__main__2015_02_26_23_12.csv.gz\n",
      "/raid/out/NEWNAME__apple_ios__main__2015_02_26_23_12.csv.gz\n",
      "/raid/out/NEWINAPP__apple_ios__main__2015_02_26_23_12.csv.gz\n",
      "/raid/out/NEWINAPP__apple_ios__main__2015_02_26_23_12.csv.gz\n"
     ]
    }
   ],
   "source": [
    "programs = \"\"\"\n",
    "NEWNAME\n",
    "NEWINAPP\n",
    "DAILYSCRAPE\n",
    "MONTHLYSCRAPE\n",
    "WEEKLYSCRAPE\n",
    "\"\"\".split()\n",
    "\n",
    "def run_function(f,RAW_FILE=RAW_FILE,OUT_BASE=OUT_BASE,COMPRESS_LEVEL=COMPRESS_LEVEL):\n",
    "    return f(RAW_FILE ,OUT_BASE,COMPRESS_LEVEL)\n",
    "\n",
    "for file in map(run_function,[globals()[x] for x in programs]):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

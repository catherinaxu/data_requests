{"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"version": "3.4.2", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"metadata": {}, "source": "# Template for all parsing\n```python\n@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef DUMPNAME(file_in,out_base,compress=1):\n    headers = [] # list of column names\n    out_filename = out_base.format('DUMPNAME')\n    \n    # Things to remmember as having \"seen\"\n    seen = set()\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            # get some attribute, cast to strting if need be\n            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n            SOMEFIELD = json.get('SOMEFIELD','')\n            if isinstance(category,str):\n                category = category.strip()\n                # double check that it's a valid field, non empty and not seen, etc.\n                if running.get(json['app_id'],'')!=SOMEFIELD and len(SOMEFIELD)>0:\n                    # SOME CODE HERE\n                    csv_writer.writerow(obs)\n    return out_filename\n```", "cell_type": "markdown"}, {"metadata": {}, "source": "# Initialize Environment", "cell_type": "markdown"}, {"metadata": {}, "source": "### Compatibility notes", "cell_type": "markdown"}, {"metadata": {}, "source": "The following code is only compatible with a python 3.4 kernel and the notebook must be opened with ipython 3.0+.\n\nThe following libraries must be installed to run this code:\n\n```pip install ujson```", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 9, "source": "from pdb import set_trace as debug\nfrom pandas.io.parsers import read_csv\nfrom IPython.parallel import Client,require\nfrom collections import Counter", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 10, "source": "import ujson\nimport datetime\nimport re\nimport gzip\nimport csv", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 11, "source": "#--> RAW_FILE_DIR with the directory that contains google_play__main.json.gz\n#--> OUT_DIR with the directory of where you want the processed files to go\n#--> DEBUG whether to print out created CSV files or not\n#--> COMPRESS_LEVEL gzip compression level. warning: severely impacts runtime.\n#--> LINE_LIMIT how many lines to iterate through in the raw file. For debugging.\nDUMP_DATE = '2015_02_26_23_12'\nRAW_FILE_DIR = '/home/cgaray/data'\nOUT_DIR = '/home/cgaray/data/out'\nDEBUG = 0\nCOMPRESS_LEVEL = 1\nLINE_LIMIT = -1\nPARALLEL = 1", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 12, "source": "# Don't change these!\nRAW_FILE = '{}/google_play__main.json.gz'.format(RAW_FILE_DIR)\nOUT_BASE = OUT_DIR+'/{}__google_play__main__'+DUMP_DATE+'.csv.gz'", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 13, "source": "def iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT):\n    with gzip.open(filename,'rt') as file_iter:\n        c = 0\n        for line in file_iter:\n            c +=1\n            if c > LINE_LIMIT and LINE_LIMIT>0:\n                break\n            if isinstance(line,str):\n                if len(line)>0:        \n                    out = ujson.loads(line)\n                    if isinstance(out,dict):\n                        if 'app_id' in out and 'timestamp' in out:\n                            yield out", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 14, "source": "# Print a couple observations to inspect daw data.\n# !zcat \"$RAW_FILE\"|head -n 10 | tail -n5", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "#Initialize Parallel Processing", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 15, "source": "if PARALLEL:\n    ipython_parallel = Client()\n    print(\"{} active computing engines\".format(len(ipython_parallel.ids)))\n\n    lbv = ipython_parallel.load_balanced_view()\n\n    map = lambda f,itertable:lbv.map(f,itertable,\\\n    block =False,\\\n    ordered =False)\n\n    @require('socket')\n    def host(dummy):\n        return socket.gethostname()\n    \n    nodes = list(lbv.map(host,ipython_parallel.ids))\n    nodes = [int(x.split('equity')[1].split(\".\")[0]) for x in nodes]\n    nodes = nodes\n\n    print(sorted(list(Counter(nodes).items())))", "outputs": [{"name": "stdout", "output_type": "stream", "text": "36 active computing engines\n[(2, 4), (6, 4), (8, 4), (12, 6), (13, 6), (17, 12)]\n"}], "cell_type": "code"}, {"metadata": {}, "source": "# DAILYRATINGS", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 16, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef DAILYRATINGS(file_in,out_base,compress=1):\n    headers = ['app_id','date','rating5','rating4','rating3','rating2','rating1']\n    out_filename = out_base.format('DAILYRATINGS')\n    unique_daily_rating = set()\n    with gzip.open(out_filename,'wt',9) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                                .strftime('%Y-%m-%d')\n            rating = json.get('ratings',None)\n            if rating:\n                obs = (json['app_id'],day,tuple(rating),)\n                if hash(obs) not in unique_daily_rating:\n                    unique_daily_rating.add(hash(obs))\n                    csv_writer.writerow([json['app_id'],day]+rating)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 17, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(DAILYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# WEEKLYRATINGS ", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 18, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef WEEKLYRATINGS(file_in,out_base,compress=1):\n    headers = ['app_id','year','week','rating5','rating4','rating3','rating2','rating1']\n    out_filename = out_base.format('WEEKLYRATINGS')\n    unique_daily_rating = set()\n    with gzip.open(out_filename,'wt',9) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            year,week = datetime.datetime.fromtimestamp(int(json['timestamp']))                                .isocalendar()[:2]\n            rating = json.get('ratings',None)\n            if rating:\n                obs = (json['app_id'],year,week)\n                if hash(obs) not in unique_daily_rating:\n                    unique_daily_rating.add(hash(obs))\n                    csv_writer.writerow([json['app_id'],year,week]+rating)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 19, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(WEEKLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# MONTHLYRATINGS", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 20, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef MONTHLYRATINGS(file_in,out_base,compress=1):\n    headers = ['app_id','year','month','rating5','rating4','rating3','rating2','rating1']\n    out_filename = out_base.format('MONTHLYRATINGS')\n    unique_daily_rating = set()\n    with gzip.open(out_filename,'wt',9) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            dateobj = datetime.datetime.fromtimestamp(int(json['timestamp']))\n            year,month = dateobj.year,dateobj.month\n            rating = json.get('ratings',None)\n            if rating:\n                obs = (json['app_id'],year,month)\n                if hash(obs) not in unique_daily_rating:\n                    unique_daily_rating.add(hash(obs))\n                    csv_writer.writerow([json['app_id'],year,month]+rating)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 21, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(MONTHLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWWEBSITE", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 22, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWWEBSITE(file_in,out_base,compress=1):\n    headers = ['app_id','date','url']\n    out_filename = out_base.format('NEWWEBSITE')\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        http = re.compile(r\"(http://)|(https://)\")\n        www = re.compile(r\"^www\\.\")\n        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n        for json in iter_json_gzip(file_in):\n            website = json.get('website','')\n            if isinstance(website,str):\n                website = http.sub('',website).lower()\n                website = www.sub('',website)\n                website = website.split(r\"/\")[0]\n                bad_str = bad.findall(website)\n                if bad_str:\n                    website = website.replace(bad_str[0],'')\n                if running.get(json['app_id'],'')!=website and len(website)>0:\n                    running[json['app_id']] = website\n                    day = datetime.datetime.\\\n                    fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d').strip()\n                    obs = (json['app_id'],day,website)\n                    csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 23, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWWEBSITE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWPRIVACYDOMAIN", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 24, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWPRIVACYDOMAIN(file_in,out_base,compress=1):\n    headers = ['app_id','date','url']\n    out_filename = out_base.format('NEWPRIVACYDOMAIN')\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        http = re.compile(r\"(http://)|(https://)\")\n        www = re.compile(r\"^www\\.\")\n        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n        for json in iter_json_gzip(file_in):\n            website = json.get('privacy_policy','')\n            if isinstance(website,str):\n                website = http.sub('',website).lower()\n                website = www.sub('',website)\n                website = website.split(r\"/\")[0]\n                bad_str = bad.findall(website)\n                if bad_str:\n                    website = website.replace(bad_str[0],'')\n                if running.get(json['app_id'],'')!=website and len(website)>0:\n                    running[json['app_id']] = website\n                    day = datetime.datetime.\\\n                    fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d').strip()\n                    obs = (json['app_id'],day,website)\n                    csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 25, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWPRIVACYDOMAIN(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWSIMILAR5", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 26, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWSIMILAR5(file_in,out_base,compress=1,number_similar = 5):\n    headers = ['app_id','date']+ ['related{}'.format(x) \\\n                                  for x in range(1,number_similar+1)]\n    out_filename = out_base.format('NEWSIMILAR{}'.format(number_similar))\n    running = {}\n    with gzip.open(out_filename,'wt',9) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            related = json.get('similar_apps',[])\n            if related:\n                related = sorted(map(str, related[:number_similar]))\n                joined = \",\".join(related)\n                if running.get(json['app_id'],'')!=joined:\n                    running[json['app_id']] = joined\n                    day = datetime.datetime.\\\n                    fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d').strip()\n                    csv_writer.writerow([json['app_id'],day]+ related)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": false}, "execution_count": 27, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWSIMILAR10", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 28, "source": "@require(NEWSIMILAR5,'ujson','datetime','re','gzip','csv')\ndef NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,10)", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 29, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWSIMILAR15", "cell_type": "markdown"}, {"metadata": {"collapsed": true, "trusted": true}, "execution_count": 30, "source": "@require(NEWSIMILAR5,'ujson','datetime','re','gzip','csv')\ndef NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,15)", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 31, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWDEVELOPER", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 32, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWDEVELOPER(file_in,out_base,compress=1):\n    headers = ['app_id','date','dev_id']\n    out_filename = out_base.format('NEWDEVELOPER')\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            developer = json.get('developer_id','')\n            if isinstance(developer,str):\n                developer = developer.strip()\n                if running.get(json['app_id'],'')!=developer and len(developer)>0:\n                    running[json['app_id']] = developer\n                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d')\n                    obs = (json['app_id'],day,developer)\n                    csv_writer.writerow(obs) \n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 33, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWDEVELOPER(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWCATEGORY", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 34, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWCATEGORY(file_in,out_base,compress=1):\n    headers = ['app_id','date','category']\n    out_filename = out_base.format('NEWCATEGORY')\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            category = json.get('category','')\n            if isinstance(category,str):\n                category = category.strip().upper()\n                if running.get(json['app_id'],'')!=category and len(category)>0:\n                    running[json['app_id']] = category\n                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d')\n                    obs = (json['app_id'],day,category)\n                    csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 35, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWCATEGORY(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWPRICE", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 36, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWPRICE(file_in,out_base,compress=1):\n    headers = ['app_id','date','price']\n    out_filename = out_base.format('NEWPRICE')\n    running = {}\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        for json in iter_json_gzip(file_in):\n            price = str(json.get('price',''))\n            if price =='0.00':\n                price = '0'\n            if running.get(json['app_id'],'')!=price and price != '.' and len(price)>0:\n                running[json['app_id']] = price\n                day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                        .strftime('%Y-%m-%d')\n                obs = (json['app_id'],day,price)\n                csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 37, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWPRICE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWVERSION", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 38, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWVERSION(file_in,out_base,compress=1):\n    headers = ['app_id','date','version']\n    out_filename = out_base.format('NEWVERSION')\n    with gzip.open(out_filename,'wt',compress,) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        running = {}\n        nowhite = re.compile(r\"\\s+\")\n        for json in iter_json_gzip(file_in):\n            version = json.get('version','')\n            if isinstance(version,str):\n                version = version.strip()\n                if len(version)>0:\n                    version = version.replace('Varies with device','varies')\n                    version = nowhite.sub('',version).lower()\n                    if running.get(json['app_id'],'')!=version and len(version)>0:\n                        running[json['app_id']] = version\n                        day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                                .strftime('%Y-%m-%d')\n                        obs = (json['app_id'],day,version)\n                        csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 39, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWVERSION(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWVERSION", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 40, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWUPDATED(file_in,out_base,compress=1):\n    headers = ['app_id','updated',]\n    out_filename = out_base.format('NEWUPDATED')\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        running = {}\n        nowhite = re.compile(r\"\\s+\")\n        for json in iter_json_gzip(file_in):\n            updated = json.get('updated','')\n            if isinstance(updated,str):\n                updated = updated.strip()\n                if running.get(json['app_id'],'')!=updated and len(updated)>0:\n                    running[json['app_id']] = updated\n                    date = datetime.datetime.strptime(updated,'%B %d, %Y')\n                    date = date.strftime('%Y-%m-%d')\n                    obs = (json['app_id'],date)\n                    csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 41, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWUPDATED(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWREQ", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 42, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWREQ(file_in,out_base,compress=1):\n    headers = ['app_id','date','requires']\n    out_filename = out_base.format('NEWREQ')\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        running = {}\n        nowhite = re.compile(r\"\\s+\")\n        and_up = re.compile(r\" and up\")\n        for json in iter_json_gzip(file_in):\n            requires = json.get('requires','')\n            if isinstance(requires,str):\n                if len(requires)>0:\n                    requires = requires.lower().strip()\n                    requires = requires.replace('varies with device','varies')\n                    requires = and_up.sub('+',requires)\n                    requires = nowhite.sub('',requires)\n                    if running.get(json['app_id'],'')!=requires and len(requires)>0:\n                        running[json['app_id']] = requires\n                        day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                                .strftime('%Y-%m-%d')\n                        obs = (json['app_id'],day,requires)\n                        csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 43, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWREQ(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": "# NEWINSTALL", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 44, "source": "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\ndef NEWINSTALL(file_in,out_base,compress=1):\n    headers = ['app_id','date','installs']\n    out_filename = out_base.format('NEWINSTALL')\n    with gzip.open(out_filename,'wt',compress) as out_file:\n        csv_writer = csv.writer(out_file)\n        csv_writer.writerow(headers)\n        nowhite = re.compile(r\"\\s+\")\n        running = {}\n        for json in iter_json_gzip(file_in):\n            installs = json.get('installs','')\n            if isinstance(installs,str):\n                installs = nowhite.sub('',installs)\n                if running.get(json['app_id'],'')!=installs and len(installs)>0:\n                    running[json['app_id']] = installs\n                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n                            .strftime('%Y-%m-%d')\n                    obs = (json['app_id'],day,installs)\n                    csv_writer.writerow(obs)\n    return out_filename", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true, "scrolled": true}, "execution_count": 45, "source": "df_debug = None\nif DEBUG:\n    df_debug = read_csv(NEWINSTALL(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\ndf_debug", "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": "# Run them all!", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 46, "source": "programs = \"\"\"\nDAILYRATINGS\nWEEKLYRATINGS\nMONTHLYRATINGS\nNEWWEBSITE\nNEWPRIVACYDOMAIN\nNEWSIMILAR5\nNEWSIMILAR10\nNEWSIMILAR15\nNEWDEVELOPER\nNEWCATEGORY\nNEWPRICE\nNEWVERSION\nNEWUPDATED\nNEWREQ\nNEWINSTALL\n\"\"\".split()\n\ndef run_function(f,RAW_FILE=RAW_FILE,OUT_BASE=OUT_BASE,COMPRESS_LEVEL=COMPRESS_LEVEL):\n    return f(RAW_FILE ,OUT_BASE,COMPRESS_LEVEL)\n\nfor file in map(run_function,[globals()[x] for x in programs]):\n    print(file)", "outputs": [{"name": "stdout", "output_type": "stream", "text": "/home/cgaray/data/out/NEWCATEGORY__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWVERSION__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWPRICE__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWREQ__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWINSTALL__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWDEVELOPER__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWPRIVACYDOMAIN__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/WEEKLYRATINGS__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/MONTHLYRATINGS__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/DAILYRATINGS__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWUPDATED__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWWEBSITE__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWSIMILAR5__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWSIMILAR10__google_play__main__2015_02_26_23_12.csv.gz\n/home/cgaray/data/out/NEWSIMILAR15__google_play__main__2015_02_26_23_12.csv.gz\n"}], "cell_type": "code"}, {"metadata": {"collapsed": false, "trusted": true}, "execution_count": 47, "source": "print(\"DONE\")", "outputs": [{"name": "stdout", "output_type": "stream", "text": "DONE\n"}], "cell_type": "code"}]}
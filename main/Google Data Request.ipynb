{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is only compatible with a python 3.4 kernel and the notebook must be opened with ipython 3.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pdb import set_trace as debug\n",
    "from pandas.io.parsers import read_csv\n",
    "from IPython.parallel import Client,require\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "import datetime\n",
    "import re\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--> RAW_FILE_DIR with the directory that contains google_play__main.json.gz\n",
    "#--> OUT_DIR with the directory of where you want the processed files to go\n",
    "#--> DEBUG whether to print out created CSV files or not\n",
    "#--> COMPRESS_LEVEL gzip compression level. warning: severely impacts runtime.\n",
    "#--> LINE_LIMIT how many lines to iterate through in the raw file. For debugging.\n",
    "DUMP_DATE = '2015_02_26_23_12'\n",
    "RAW_FILE_DIR = '/data'\n",
    "OUT_DIR = '/raid/out'\n",
    "DEBUG = 0\n",
    "COMPRESS_LEVEL = 1\n",
    "LINE_LIMIT = -1\n",
    "PARALLEL = 1\n",
    "PIGZ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't change these!\n",
    "RAW_FILE = '{}/google_play__main.json.gz'.format(RAW_FILE_DIR)\n",
    "OUT_BASE = OUT_DIR+'/{}__google_play__main__'+DUMP_DATE+'.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PIGZ:\n",
    "    @require(gzip)\n",
    "    def iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT):\n",
    "        return gzip.iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT)\n",
    "        \n",
    "else:\n",
    "    @require(gzip,ujson)\n",
    "    def iter_json_gzip(filename,LINE_LIMIT=LINE_LIMIT):\n",
    "        with gzip.open(filename,'rt') as file_iter:\n",
    "            for c,line in enumerate(file_iter):\n",
    "                if c > LINE_LIMIT and LINE_LIMIT>0:\n",
    "                    break\n",
    "                if isinstance(line,str):\n",
    "                    if len(line)>0:        \n",
    "                        out = ujson.loads(line)\n",
    "                        if isinstance(out,dict):\n",
    "                            if 'app_id' in out and 'timestamp' in out:\n",
    "                                yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a couple observations to inspect daw data.\n",
    "# !zcat \"$RAW_FILE\"|head -n 10 | tail -n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initialize Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PARALLEL:\n",
    "    ipython_parallel = Client()\n",
    "    print(\"{} active computing engines\".format(len(ipython_parallel.ids)))\n",
    "\n",
    "    lbv = ipython_parallel.load_balanced_view()\n",
    "\n",
    "    map = lambda f,itertable:lbv.map(f,itertable,\\\n",
    "    block =False,\\\n",
    "    ordered =False)\n",
    "\n",
    "    @require('socket')\n",
    "    def host(dummy):\n",
    "        return socket.gethostname()\n",
    "    \n",
    "    nodes = list(lbv.map(host,ipython_parallel.ids))\n",
    "\n",
    "    print(dict(Counter(nodes).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILYRATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def DAILYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('DAILYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .strftime('%Y-%m-%d')\n",
    "            rating = json.get('ratings',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],day,tuple(rating),)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],day]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(DAILYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEKLYRATINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def WEEKLYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','year','week','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('WEEKLYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            year,week = datetime.datetime.fromtimestamp(int(json['timestamp']))                                .isocalendar()[:2]\n",
    "            rating = json.get('ratings',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],year,week)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],year,week]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(WEEKLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTHLYRATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def MONTHLYRATINGS(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','year','month','rating5','rating4','rating3','rating2','rating1']\n",
    "    out_filename = out_base.format('MONTHLYRATINGS')\n",
    "    unique_daily_rating = set()\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            dateobj = datetime.datetime.fromtimestamp(int(json['timestamp']))\n",
    "            year,month = dateobj.year,dateobj.month\n",
    "            rating = json.get('ratings',None)\n",
    "            if rating:\n",
    "                obs = (json['app_id'],year,month)\n",
    "                if hash(obs) not in unique_daily_rating:\n",
    "                    unique_daily_rating.add(hash(obs))\n",
    "                    csv_writer.writerow([json['app_id'],year,month]+rating)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(MONTHLYRATINGS(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWWEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWWEBSITE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','url']\n",
    "    out_filename = out_base.format('NEWWEBSITE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        http = re.compile(r\"(http://)|(https://)\")\n",
    "        www = re.compile(r\"^www\\.\")\n",
    "        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n",
    "        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            website = json.get('website','')\n",
    "            if isinstance(website,str):\n",
    "                website = http.sub('',website).lower()\n",
    "                website = www.sub('',website)\n",
    "                website = website.split(r\"/\")[0]\n",
    "                bad_str = bad.findall(website)\n",
    "                if bad_str:\n",
    "                    website = website.replace(bad_str[0],'')\n",
    "                if running.get(json['app_id'],'')!=website and len(website)>0:\n",
    "                    running[json['app_id']] = website\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    obs = (json['app_id'],day,website)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWWEBSITE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWPRIVACYDOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWPRIVACYDOMAIN(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','url']\n",
    "    out_filename = out_base.format('NEWPRIVACYDOMAIN')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        http = re.compile(r\"(http://)|(https://)\")\n",
    "        www = re.compile(r\"^www\\.\")\n",
    "        ending = re.compile(r\"\\.(a-z)(.*?)^\")\n",
    "        bad = re.compile(r\"\\.[a-z]+(.*?)$\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            website = json.get('privacy_policy','')\n",
    "            if isinstance(website,str):\n",
    "                website = http.sub('',website).lower()\n",
    "                website = www.sub('',website)\n",
    "                website = website.split(r\"/\")[0]\n",
    "                bad_str = bad.findall(website)\n",
    "                if bad_str:\n",
    "                    website = website.replace(bad_str[0],'')\n",
    "                if running.get(json['app_id'],'')!=website and len(website)>0:\n",
    "                    running[json['app_id']] = website\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    obs = (json['app_id'],day,website)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWPRIVACYDOMAIN(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIMILAR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWSIMILAR5(file_in,out_base,compress=1,number_similar = 5):\n",
    "    headers = ['app_id','date']+ ['related{}'.format(x) \\\n",
    "                                  for x in range(1,number_similar+1)]\n",
    "    out_filename = out_base.format('NEWSIMILAR{}'.format(number_similar))\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',9) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            related = json.get('similar_apps',[])\n",
    "            if related:\n",
    "                related = sorted(map(str, related[:number_similar]))\n",
    "                joined = \",\".join(related)\n",
    "                if running.get(json['app_id'],'')!=joined:\n",
    "                    running[json['app_id']] = joined\n",
    "                    day = datetime.datetime.\\\n",
    "                    fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d').strip()\n",
    "                    csv_writer.writerow([json['app_id'],day]+ related)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIMILAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(NEWSIMILAR5,'ujson','datetime','re','gzip','csv')\n",
    "def NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n",
    "    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR10(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWSIMILAR15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(NEWSIMILAR5,'ujson','datetime','re','gzip','csv')\n",
    "def NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL):\n",
    "    return NEWSIMILAR5(RAW_FILE,OUT_BASE,COMPRESS_LEVEL,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWSIMILAR15(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWDEVELOPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWDEVELOPER(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','dev_id']\n",
    "    out_filename = out_base.format('NEWDEVELOPER')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            developer = json.get('developer_id','')\n",
    "            if isinstance(developer,str):\n",
    "                developer = developer.strip()\n",
    "                if running.get(json['app_id'],'')!=developer and len(developer)>0:\n",
    "                    running[json['app_id']] = developer\n",
    "                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d')\n",
    "                    obs = (json['app_id'],day,developer)\n",
    "                    csv_writer.writerow(obs) \n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWDEVELOPER(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWCATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWCATEGORY(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','category']\n",
    "    out_filename = out_base.format('NEWCATEGORY')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            category = json.get('category','')\n",
    "            if isinstance(category,str):\n",
    "                category = category.strip().upper()\n",
    "                if running.get(json['app_id'],'')!=category and len(category)>0:\n",
    "                    running[json['app_id']] = category\n",
    "                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d')\n",
    "                    obs = (json['app_id'],day,category)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWCATEGORY(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWPRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWPRICE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','price']\n",
    "    out_filename = out_base.format('NEWPRICE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            price = str(json.get('price',''))\n",
    "            if price =='0.00':\n",
    "                price = '0'\n",
    "            if running.get(json['app_id'],'')!=price and price != '.' and len(price)>0:\n",
    "                running[json['app_id']] = price\n",
    "                day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                        .strftime('%Y-%m-%d')\n",
    "                obs = (json['app_id'],day,price)\n",
    "                csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWPRICE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWVERSION(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','version']\n",
    "    out_filename = out_base.format('NEWVERSION')\n",
    "    with gzip.open(out_filename,'wt',compress,) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        running = {}\n",
    "        nowhite = re.compile(r\"\\s+\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            version = json.get('version','')\n",
    "            if isinstance(version,str):\n",
    "                version = version.strip()\n",
    "                if len(version)>0:\n",
    "                    version = version.replace('Varies with device','varies')\n",
    "                    version = nowhite.sub('',version).lower()\n",
    "                    if running.get(json['app_id'],'')!=version and len(version)>0:\n",
    "                        running[json['app_id']] = version\n",
    "                        day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .strftime('%Y-%m-%d')\n",
    "                        obs = (json['app_id'],day,version)\n",
    "                        csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWVERSION(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWUPDATED(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','updated',]\n",
    "    out_filename = out_base.format('NEWUPDATED')\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        running = {}\n",
    "        nowhite = re.compile(r\"\\s+\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            updated = json.get('updated','')\n",
    "            if isinstance(updated,str):\n",
    "                updated = updated.strip()\n",
    "                if running.get(json['app_id'],'')!=updated and len(updated)>0:\n",
    "                    running[json['app_id']] = updated\n",
    "                    date = datetime.datetime.strptime(updated,'%B %d, %Y')\n",
    "                    date = date.strftime('%Y-%m-%d')\n",
    "                    obs = (json['app_id'],date)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWUPDATED(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWREQ(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','requires']\n",
    "    out_filename = out_base.format('NEWREQ')\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        running = {}\n",
    "        nowhite = re.compile(r\"\\s+\")\n",
    "        and_up = re.compile(r\" and up\")\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            requires = json.get('requires','')\n",
    "            if isinstance(requires,str):\n",
    "                if len(requires)>0:\n",
    "                    requires = requires.lower().strip()\n",
    "                    requires = requires.replace('varies with device','varies')\n",
    "                    requires = and_up.sub('+',requires)\n",
    "                    requires = nowhite.sub('',requires)\n",
    "                    if running.get(json['app_id'],'')!=requires and len(requires)>0:\n",
    "                        running[json['app_id']] = requires\n",
    "                        day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                                .strftime('%Y-%m-%d')\n",
    "                        obs = (json['app_id'],day,requires)\n",
    "                        csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWREQ(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWINSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWINSTALL(file_in,out_base,compress=1):\n",
    "    headers = ['app_id','date','installs']\n",
    "    out_filename = out_base.format('NEWINSTALL')\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        nowhite = re.compile(r\"\\s+\")\n",
    "        running = {}\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            installs = json.get('installs','')\n",
    "            if isinstance(installs,str):\n",
    "                installs = nowhite.sub('',installs)\n",
    "                if running.get(json['app_id'],'')!=installs and len(installs)>0:\n",
    "                    running[json['app_id']] = installs\n",
    "                    day = datetime.datetime.fromtimestamp(int(json['timestamp']))\\\n",
    "                            .strftime('%Y-%m-%d')\n",
    "                    obs = (json['app_id'],day,installs)\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWINSTALL(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWNAME(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'name'] # list of column names\n",
    "    out_filename = out_base.format('NEWNAME')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            name = json.get('app_name','')\n",
    "            if isinstance(name,str):\n",
    "                name = name.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=name and len(name)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = name\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', '')))), name]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWNAME(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWINAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def NEWINAPP(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'date', 'has_inapp'] # list of column names\n",
    "    out_filename = out_base.format('NEWINAPP')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            name = str(bool(json.get('hasInAppPurchases','0')))\n",
    "            if isinstance(name,str):\n",
    "                name = name.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=name and len(name)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = name\n",
    "                    obs = [json['app_id'], str(datetime.date.fromtimestamp(int(json.get('timestamp', '')))), name]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(NEWINAPP(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAILYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def DAILYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'day'] # list of column names\n",
    "    out_filename = out_base.format('DAILYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            day = str(datetime.date.fromtimestamp(json.get('timestamp','')).day)\n",
    "            if isinstance(day,str):\n",
    "                day = day.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=day and len(day)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = day\n",
    "                    obs = [json['app_id'], day]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(DAILYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTHLYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def MONTHLYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'month'] # list of column names\n",
    "    out_filename = out_base.format('MONTHLYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            month = str(datetime.date.fromtimestamp(json.get('timestamp','')).month)\n",
    "            if isinstance(month,str):\n",
    "                month = month.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=month and len(month)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = month\n",
    "                    obs = [json['app_id'], month]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(MONTHLYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEKLYSCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@require(iter_json_gzip,'ujson','datetime','re','gzip','csv')\n",
    "def WEEKLYSCRAPE(file_in,out_base,compress=1):\n",
    "    headers = ['app_id', 'year', 'week'] # list of column names\n",
    "    out_filename = out_base.format('WEEKLYSCRAPE')\n",
    "    running = {}\n",
    "    with gzip.open(out_filename,'wt',compress) as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(headers)\n",
    "        for json in iter_json_gzip(file_in):\n",
    "            # get some attribute, cast to strting if need be\n",
    "            # what you're \"remmebering\" can also be a tuple (not list, they're not hashable)\n",
    "            date = (str(datetime.date.fromtimestamp(json.get('timestamp','')).year), str(datetime.date.fromtimestamp(json.get('timestamp','')).isocalendar()[1]))\n",
    "            if isinstance(date,str):\n",
    "                date = date.strip()\n",
    "                # double check that it's a valid field, non empty and not seen, etc.\n",
    "                if running.get(json['app_id'],'')!=date and len(date)>0:\n",
    "                    # SOME CODE HERE\n",
    "                    running[json['app_id']] = date\n",
    "                    obs = [json['app_id'], date[0], date[1]]\n",
    "                    csv_writer.writerow(obs)\n",
    "    return out_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_debug = None\n",
    "if DEBUG:\n",
    "    df_debug = read_csv(WEEKLYSCRAPE(RAW_FILE,OUT_BASE,COMPRESS_LEVEL),compression='gzip',header=0)\n",
    "df_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "programs = \"\"\"\n",
    "NEWNAME\n",
    "NEWINAPP\n",
    "DAILYSCRAPE\n",
    "MONTHLYSCRAPE\n",
    "WEEKLYSCRAPE\n",
    "\"\"\".split()\n",
    "\n",
    "def run_function(f,RAW_FILE=RAW_FILE,OUT_BASE=OUT_BASE,COMPRESS_LEVEL=COMPRESS_LEVEL):\n",
    "    return f(RAW_FILE ,OUT_BASE,COMPRESS_LEVEL)\n",
    "\n",
    "for file in map(run_function,[globals()[x] for x in programs]):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
